# clustering
按照如下步骤进行聚类算法实验：
1)	读取数据集。进行可视化显示，如：
 
2)	数据处理。对content数据进行数据处理，包括将文章内容的专有名词和数字删除，将英文大写转化为小写，并还原英文单词原型等。可利用Python中nltk语料库进行处理。词干提取过程如下：
a. 将content列存在空值的行删除；
b. 利用正则表达式将数字和‘.’删除，利用nltk库中的英文分词算法(Porterstemmer)对英文进行分词处理，能够实现还原英文单词原型，比如boys变为boy等，利用nltk库中pos_tag进行词性标注，对标签为NNP（Proper noun, singular）的内容进行删除（部分专有名词仍会保留）。
 
3)	创建词汇表。用于统计所有文章中单词以及出现的次数，用于之后单词的矢量化。主要从collections库中引入Counter来统计单词以及次数，并将其保存到新的csv文件中。csv文件的样式如下：
 
4)	词汇表的简化。处理源自互联网的文本数据时面临的一个挑战是，非单词，如字符和符号的组合（例如，“@username”，“#hashtags”，与省略号连在一起的单词，如“well...”）。在这里只保留nltk完整英语语料库中的单词（from nltk.corpus import words），来完成词汇表的最终简化。
提示：检查词汇表的最终简化后是否含有nan值，有的话删除，否则后续处理会报错。这将总词汇量从375943减少到38517。它处理了词汇表中的一部分噪音。
5)	对词汇进行矢量化。简单来说，TfIdf（术语频率反转文档频率）矢量化器为每篇文章中的每个单词给出一个值，并按整个语料库中的单词频率加权。利用了TfidfVectorizer库进行矢量化处理（from sklearn.feature_extraction.text import TfidfVectorizer），得到词频矩阵。
逆文档频率是从整个数据集中的单词频率派生的分母。以“perspicacious”这个词为例，因为它在英语中很少看到。由于这种稀缺性，它的反向文档频率很低。如果它在一篇文章中出现15次，它的Tf值或分子会很高。因此，它的TfIdf值产生一个很高的数字。
为了剔除个别单词的影响，选取词汇表count中第40分位数和第99.5分位数之间的词汇。当然分位数的选取希望自己决定，这里仅供参考。
第40分位的部分词汇信息如下：
 
可知第40分位数的部分词汇在所有文章中只有9次出现—非常低，因此几乎不提供信息。最终产生一个词频矩阵，大小为(188673, 22453)，其中188673是文章的数量，22453是词汇表count中第40分位数和第99.5分位数之间的词汇数量。
6)	降维。将22453维矩阵减少到100个维度，也可以尝试其它数值。
7)	聚类。对降维后的词频矩阵采用聚类的方法（这里K选取为10，具体分类数自行选取）。可将聚类标签加入原数据集中便于后续的可视化处理。
8)	结果可视化。进行一系列可视化，例如：
 
3.	可视化分析。对聚类结果进行可视化展示，比如同一个publication的文章被分配到每个簇的百分比，同一类中各个publication的占比等等。
4.	量化评估。对聚类结果进行分析与评估（比如轮廓系数等），并解释数据和图表的含义，能够得出一些结论。
5.	实验描述。要求把实验过程以及参数的选取原因描述清楚，实验结果分析结合图表呈现，成绩由实验过程描述、实验结果分析和上交代码综合评定。
6.	总结归纳。要求能够理解文本聚类的一般思路，包括数据的处理以及矢量化等。


